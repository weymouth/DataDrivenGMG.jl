\documentclass[review]{elsarticle}

\usepackage{lineno,hyperref}
\modulolinenumbers[5]

\journal{Journal of Computers and Fluids}

%% `Elsevier LaTeX' style
\bibliographystyle{elsarticle-num}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath}
%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\begin{frontmatter}

\title{Data-Driven Multi-grid Solver for Accelerated \\ Pressure Projection}

%% Group authors per affiliation:
\author{Gabriel D Weymouth}
\address{Engineering and Physical Sciences, University of Southampton, Southampton, UK}
\address{Data-Centric Engineering, Alan Turing Institute, London, UK}
\ead[url]{https://weymouth.github.io/}

\begin{abstract}
Pressure projection is the single most computationally expensive step in an unsteady incompressible fluid simulation. This work discusses the potential of data-driven methods to accelerate the approximate solution of the Poisson equation at the heart of pressure projection, linking Multi-grid methods to linear convolutional encoder-decoder networks. A high-speed Jacobi-like parameterized smoother is developed and optimized using automatic differentiation on the recursive log-residual loss function. The new method is found to accelerate classic Multi-grid methods by a factor of 2-3 with no loss of accuracy on eleven 2D and 3D flow cases including cases with dynamic immersed solid boundaries. The optimal parameters are found to transfer nearly 100\% effectiveness as the resolution is increased, providing a robust approach for accelerated pressure projection of general flows.
\end{abstract}

\begin{keyword}
pressure projection, linear algebra, data-driven
\end{keyword}

\end{frontmatter}

\section{Introduction}

Pressure projection is a bottleneck in high-speed unsteady incompressible flow solvers. Constructing approximate solutions the discrete pressure Poisson system is the most expensive part of each time-step as the elliptic equation requires communication throughout the computational domain instead of being confined to a local characteristic. As such, the proportional cost only grows in massively parallel simulations due to the required communication across processes. Methods such as artificial-compressibility \cite{he2002comparison}, smooth-particle-hydrodynamics \cite{kiara2013sph}, and particle-in-cell \cite{jiang2017angular} attempt to side-step this computational cost by modelling or otherwise under-resolving the pressure evolution compared to the fluid's momentum. However, these approaches lead to significant errors in pressure forces, thereby making them unsuitable for many applications or requiring explicit pressure corrections, \cite{kiara2013sph}.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/drawing}
    \caption{Sketch of a convolutional encoder-decoder network and/or a geometric multi-grid V-cycle. Data is restricted down to a reduced size for faster processing before being pushed back up to the full size. A two-level V-cycle is shown, but this process repeats recursively between the largest and smallest grid levels.}
    \label{fig:multigrid}
\end{figure}

Advances in data-driven acceleration of flow solvers have mainly focused on reducing simulation resolution using data-assimilation and data-driven turbulence closures \cite{asch2016data,BECK2019108910,maulik_san_rasheed_vedula_2019,ling_kurzawski_templeton_2016,font2021deep} without focusing on projection explicitly. Many of these approaches utilize Convolutional Neural Networks (CNN), Figure \ref{fig:multigrid}, which have been successfully applied in other fluids applications such as super-resolution \cite{liu2020deep} and full field regression models \cite{bhatnagar2019prediction}. CNNs have inherent translational symmetry and encoder-decoder architectures reduce the number of variables before expanding again, both of which help constrain learning capacity and generalize the trained networks to unseen prediction cases. 

A few recent studies have applied machine-learning to accelerate the critical projection step itself \cite{ozbay2021poisson,Xiao2020,ajuria2020} using CNNs to predict the pressure field given the projection source term. All of these method assume a uniform grid without internal solid geometries, although \cite{ozbay2021poisson} uses a decomposition to handle different \textit{domain} boundary conditions and grid aspect ratios in 2D. Unfortunately, the methods can't be applied to most simulation problems and the results are still somewhat qualitative, with pressure errors greater than 10\% throughout much of the domain \cite{ozbay2021poisson}. 

A key insight is that classic Geometric Multi-Grid (GMG) methods \cite{briggs2000multigrid} are simply \textit{linear} convolutional encoder-decoder networks, Figure \ref{fig:multigrid}, prime for data-driven acceleration without any loss of accuracy in the resulting pressure field. While there is a long history of optimizing multi-grid methods (both geometric \cite{oosterlee2003genetic} and algebraic \cite{brezina2006adaptive}), few of these have taken a direct data-driven approach. Recent work \cite{KATRUTSA2020112524,greenfeld2019learning} has shown promising initial results in constructing data-driven restriction and prolongation operators, but both use idealized systems and do not show overall speed-up compared to classic methods, so their utility in accelerating real pressure projection problems is untested.

Building on this foundation, this work develops a simple and accurate data-driven projection method which outperforms classic geometric multi-grid on eleven 2D and 3D benchmarks including those with immersed solid boundaries. Moreover, since the new approach is linear in the pressure and the nonlinear dependence on the matrix coefficients are scale-invariant, the data-optimized parameters generalize extremely well to new flows and simulation resolutions.

\section{Linear System Description}

The discrete Poisson equation in the projection step is defined as
\begin{equation}\label{eq:axb}
    A x = b
\end{equation}
where $x$ is the desired pressure field vector, $b$ is the source term (proportional to the divergence of the velocity field to be projected), and $A$ is the Poisson matrix. For conservative Poisson equations, $A$ is symmetric, has zero-sum rows and columns, is negative semi-definite, and has a single zero eigenvalue. The matrix is also extremely sparse. While the solution vector size $N$ may easily be $10^6-10^8$, a second-order scheme on structured grids in $M$ dimensions result in only $M$ non-zero sub-diagonals.

Iterative methods solve equation \ref{eq:axb} by updating an approximate solution $x^k$ to a new solution $x^{k+1}$ with reduced error. As the problem is linear, the equation for the update $\epsilon$ is simply
\begin{equation}\label{eq:aer} 
    A \epsilon^k = r^k \equiv b - Ax^k
\end{equation}
where $r$ is the residual. In practise, only an approximate solution for $\epsilon$ is obtained, after which the solution and residual are incremented
\begin{equation}\label{eq:increment}
    x^{k+1} = x^k+\epsilon^k, \quad r^{k+1} = r^k-A\epsilon^k.
\end{equation}
This process is iterated until the residual is reduced to a specified level.

Geometric Multi-Grid (GMG) methods are among the fastest iterative solution approaches for variable coefficient discrete pressure Poisson equations. A single iteration of GMG is called a V-cycle and consists of (i) a solution precondition step, (ii) residual restriction down to the reduced-size problem, (iii) approximate solution of this problem, (iv) solution prolongation back up to correct the fine grid, and (v) solution smoothing. This process is recursive, being applied to the reduced-size problems until they are small enough to be solved directly, resulting in $O(N\log N)$ computational cost overall.
% \begin{equation}
%     r_c = R r \quad\rightarrow\quad A_c \epsilon_c = r_c \quad\rightarrow\quad \epsilon = P \epsilon_c
% \end{equation}
% where $R$ is the $N_c \times N$ restriction operator, $P$ is the $N \times N_c$ prolongation operator 
The first four steps of the V-cycle distribute the residual throughout the domain, which enables simple and relatively fast stationary methods such as Gauss-Sidel or Successive Over Relation (SOR) to effectively smooth the local error in the projected solution. The V-cycle can be repeated until convergence or used as an efficient preconditioner for another iterative solver such as Conjugate Gradient. 

\section{Data-driven Accelerated Projection Method}

As \textit{any} element of the residual of the elliptic Poisson system potentially influences \textit{every} element of the update, $O(N\log N)$ is the best possible solution time scaling. However, data-driven methods can still be used to accelerate pressure projection by speeding-up and increasing the residual reduction of each V-cycle iteration.

The linearity of equation \ref{eq:aer} is a critical property which the data-driven method must maintain. Consider that the modes of the residual will change as the solver converges, meaning that a nonlinear method could not be applied iteratively. However, this linearity is only with respect to the update and residual fields. The operators of the MG method can all potentially be (implicit or explicit) nonlinear functions of $A$, embedding information about the discrete problem and boundary conditions. 

Which MG operation is the best candidate for such an approach? As discussed in the introduction, significant effort has focused on optimization of the prolongation operator and its transpose restriction operator. The prolongation operator is an \textit{implicit} function of $A$, making their construction difficult in algebraic settings and their optimization mathematically and numerically complex even in geometric settings \cite{KATRUTSA2020112524,greenfeld2019learning}. Note that super-resolution operators are related to prolongation, but would violate the linearity constraint. 

Another option is the precondition operator, but a simple Jacobi preconditioner
\begin{equation}
    \epsilon = D^{-1}r
\end{equation}
where $D$ is the diagonal of $A$, is sufficient for the V-cycle to distribute the residual throughout the domain and is as fast as possible; being a single Schur (broadcasted) vector product. Therefore, for the remainder of the paper we will use a simple Jacobi preconditioner and uniform pooling/distribution for restriction/prolongation and focus on a parameterized smoothing operator.

While more complex options are certainly possible, this work uses a simple parameterized Jacobi-like smoother
\begin{equation}\label{eq:smooth}
    \epsilon = \tilde A^{-1}r
\end{equation}
where $\tilde A^{-1}=f(A\,|\theta)$ is an approximate matrix-inverse with the same sparsity as $A$ and parameter vector $\theta$. The matrix $A$ is constant during the projection step (and often for an entire simulation), meaning $\tilde A^{-1}$ can be computed and stored ahead of time. And unlike Gauss-Sidel or SOR smoothers, the matrix-vector multiplication of equation \ref{eq:smooth} requires no back-propagation and so can be vectorized in serial or parallel implementations, offering a significant potential speed-up. 

There are few constraints on $\tilde A^{-1}$: it must have units of $A^{-1}$, and symmetry of $A$ implies the approximate inverse should also be symmetric. For simplicity, the diagonal and off-diagonal coefficients of $\tilde A^{-1}$ are constructed independently 
\begin{equation}\label{eq:approxinv}
    \tilde a^{-1}_{ii} = \frac{f_d(a_{ii}/s\,|\theta_d)}{a_{ii}} , \quad
    \tilde a^{-1}_{ij} = \frac{f_o(a_{ij}/s\,|\theta_o)}{a_{ii}+a_{jj}}
\end{equation}
where function inputs are scaled by the maximum off-diagonal $s=\max(A-D)$ and the outputs are scaled by the diagonal elements. Note that $f_o(0)=0$ is required to maintain the sparsity of $\tilde A^{-1}$ and that a Jacobi smoother would use a diagonal function $f_d=1$ and off-diagonal function $f_o=0$. With the optimization problem now reduced to two normalized single-variable functions, the specific choice of parameterization is not critical and a simple quadratic is chosen for $f_d$ and $f_o$. Using higher-order polynomials, splines, and interpolating kernels did not significantly change the results.

The parameterized smoother is optimized for a set of training data $X=\{A,x^k,b\}$. The loss function is simply the reduction in the norm of the residual after a V-cycle iteration $L = \log_{10}(|r^{k+1}| / |r^{k}|)$, which means that example pressure \textit{solutions} are not required. This loss is averaged over each iteration and each example in the data-set. The optimal parameters are defined as
\begin{equation}
    \hat\theta = \min_\theta L(\theta\, |X)
\end{equation}
The residual loss function is a highly nonlinear recursive function of the parameters, but automatic differentiation (AD) is used to determine the gradient $\nabla_\theta L$ and Hessian $H_{\theta\theta} L$ in a Newton's method optimizer  \cite{mogensen2018optim,RevelsLubinPapamarkou2016}. The use of AD allows this data-driven solver to extend to an arbitrary number of grid levels, avoiding inaccurate and potentially unstable finite differences.

The data-driven solver and all test cases presented in the following sections are implemented in the Julia programming language \cite{bezanson2017julia} and available for reproduction \cite{weymouth2021julia}.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/2D-static.png}
        \caption{2D-static}
        \label{fig:2D-static}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/2D-dipole.png}
        \caption{2D-dipole}
        \label{fig:2D-mu}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/2D-sphere.png}
        \caption{2D-sphere}
        \label{fig:2D-sphere}
    \end{subfigure}
    \caption{Sample solutions from the data-sets of three of the six synthetic cases.}
    \label{fig:synthetic cases}
\end{figure}
\begin{figure}
    \centering
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/crossloss.png}
        \caption{single-cycle residual reduction}
        \label{fig:cross plot}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/synthetic_timing.png}
        \caption{solver time}
        \label{fig:synthetic time}
    \end{subfigure}
    \caption{a) Residual reduction factor $|r_{k+1}|/|r_k|$ over a single Multi-grid V-cycle on the `test' case after tuning using the `train' case. Training case `union' refers to the smoother trained on all of the synthetic cases. (b) Time to reduce pressure residual by $10^{-3}$ for classical and parameterized smoothers on each synthetic case. Time is relative to the time of a single V-cycle using the Jacobi smoother.}
    \label{fig:synthetic results}
\end{figure}

\section{Synthetic Discrete Poisson System Results}

A set of six synthetic projection cases were constructed to establish the characteristics of the data-driven approach. The three 2D cases are shown in Figure~\ref{fig:synthetic cases}, and each has a matching 3D case. The `static' cases have a hydrostatic gradient throughout the domain, with the direction and magnitude randomized. The `dipole' and `sphere' cases have a randomized dipole/sphere within a quiescent flow. All cases are initialized with $x^0=0$.
%, and all cases feature initially localized residuals; i.e. $r^0\ne0$ only in a fraction of the cells, but $x$ must be updated throughout the domain, starkly illustrating the elliptic nature of the projection step.
Neumann boundary conditions are applied on all boundaries, including the boundary of the immersed sphere, through adjustment of the $A$ matrix coefficients which are otherwise uniform. 

Figure~\ref{fig:cross plot} characterizes the generalization of the parameterized smoother's residual reduction over a single V-cycle on 100 randomized `test' case examples after optimization using 100 examples of the `train' case. Each example uses a grid size of $N=32^M$ points ($N=32^2$ in 2D and $N=32^3$ in 3D) as testing with other resolutions found essentially no change in the performance. V-cycle residual reduction using a Jacobi-smoother is shown for comparison, as this is the fastest option and the initial condition for the data-driven method. As expected, the performance is best when testing and training on the same case, with a single V-cycle reducing the residual by $|r_{k+1}|/|r_k|=10^{-1.19}$ on the 2D-sphere case down to $10^{-1.68}$ on the 2D-dipole case. Note that despite identical $A$ matrices in the static and dipole cases, the data-driven method find completely different optimal $\tilde A^{-1}$ matrices, as shown by their different generalization performance. While the performance for the simple static cases generalizes poorly, the solver trained on the 2D-sphere generalizes essentially as well as the solver trained on the `union' of the all data. The `union' data-driven solver improves the performance between 105\% (2D-circle) and 2205\% (3D-static) compared to the Jacobi-smoothed V-cycle.

The acceleration of the `union'-trained data-driven projection method is evaluated on new example data of a different size, $n=64^M$ points. Figure~\ref{fig:synthetic time} shows the time required to reduce the residual by $10^{-3}$. The time is scaled by the time required to run a single V-cycle using a Jacobi smoother but the Jacobi smoother isn't a viable option since it can require hundreds of V-cycles to solve even these simple projection problems. While the Jacobi-like parameterized smoother is only around 30-50\% slower per V-cycle, it converges in only 1-3 V-cycles in all cases. This results in a 80-210\% speed-up (mean: 133\%) relative to optimized serial Gauss-Sidel and SOR smoothers. This speed-up will be even more favorable in parallel computation as the new smoother operates directly on the local residual without back-propagation, eliminating any additional communication.

\begin{figure}
    \centering
    \begin{subfigure}[a]{\textwidth}
        \centering 
        \includegraphics[width=\textwidth]{figures/circletriple.png}
        \caption[-1mm]{Circle flow}
    \end{subfigure}
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/donuttriple.png}
        \caption{Slice of 3D torus (donut) flow}
    \end{subfigure}
    \begin{subfigure}[c]{0.6\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/TGVtriple.png}
        \caption{Slice of 3D Taylor-Green Vortex}
    \end{subfigure}
    \begin{subfigure}[c]{0.6\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/wingtriple.png}
        \caption{Flapping wing}
    \end{subfigure}
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/sharktriple.png}
        \caption{Swimming shark}
    \end{subfigure}
    \hfill
    \caption{Snapshots from the five flow simulation cases showing the pressure solution (left), the initial residual (middle), and the residual after a single V-cycle (right) using the data-driven projection method. The Taylor-Green Vortex and torus flow are 3D, the flapping wing is dynamic without a background flow and the swimming shark is a deforming geometry. All residuals are visualized using $\log_{10}\text{abs}(r)$ on the same scale, $10^{-6} \ldots 10^{-1}$.}
    \label{fig:simulation cases}
\end{figure}

\section{Unsteady Incompressible Simulation Results}

Five unsteady incompressible simulation cases were used to further characterize the accelerated projection method, Figure \ref{fig:simulation cases}. All simulations use the same validated Cartesian-grid method \cite{maertens2015accurate} but feature significantly different physics, i.e. 2D and 3D simulations with and without background flows, immersed geometries, body motion, and body deformation. The computational grids are of different sizes and aspect ratios, and the V-cycle is extended recursively down to the smallest possible grid in all cases. 

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/scaleloss.png}
        \caption{single-cycle residual reduction}
        \label{fig:scaled loss}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/crosscount.png}
        \caption{solver time}
        \label{fig:simulation time}
    \end{subfigure}
        \caption{(a) Residual reduction over a single Multi-grid V-cycle on the full-resolution case. The `transfer' smoother has been trained on the union of the synthetic data sets from Figure~\ref{fig:synthetic cases}. The $\frac 18, \frac 14, \frac 12$ smoothers have been trained on simulations with the indicated reduced resolution \textit{in each spacial and temporal dimension}. (b) Time to reduce pressure residual by $10^{-3}$ for classical and parameterized smoothers on each simulation case. Time is relative to the time of a single V-cycle using the Jacobi smoother.}
        \label{fig:tuned simulation}
\end{figure}

Figure \ref{fig:scaled loss} shows that the new data-driven method trained on the synthetic cases of the previous section reduces the residual on these unseen simulation cases more effectively than a V-cycle using Jacobi smoothing. Indeed, this `transfer' smoother achieves 85-95\% of the residual reduction of a smoother optimized for each case. Even more promising is that training on reduced-size simulations closes that small gap almost completely. Training on a $1/4^\text{th}$-scale simulation is fast, requiring only $N_{1/4} = N/4^M$ points (N/16 in 2D and N/64 in 3D) and $\approx 1/4$ the number of time steps, while achieving 99-100\% of the residual reduction of training with full-scale data. Such an \textit{auto-tuning} approach enables a highly effective data-driven projection method to be developed for any simulation approach with very little overhead.

Figure \ref{fig:simulation time} shows that this data-driven method greatly acceleration the projection step relative to the classic MG methods on these real flow cases. The solver tuned on the reduced resolution cases provides 112 - 230\% acceleration, and even the transfer case provides a mean acceleration of 120\%. The pressure solutions, initial residual field, and the residual after a single data-driven V-cycle are shown in \ref{fig:simulation cases}. The residual decreases throughout the domain, including at the external and internal boundaries. As such, the new data-driven method is shown to produce uniformly valid solutions despite being trained to optimize the global residual loss.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/diag_fun.png}
        \caption{diagonal functions $f_d$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/lower_fun.png}
        \caption{off-diagonal functions $f_o$}
    \end{subfigure}
        \caption{(a) Diagonal and (b) off-diagonal parameterized functions after optimization on the 1/4-resolution case. The `transfer' smoother was tuned on the synthetic cases and `Jacobi' is shown for comparison.}
        \label{fig:tuned inverse}
\end{figure}

Finally, the parameterized $f_d,\,f_o$ functions used in the approximate inverse equation \ref{eq:approxinv} are shown in Figure \ref{fig:tuned inverse}. It is interesting to note that the diagonal functions are all somewhat centered on one, the value for the Jacobi smoother. We also note that the `wing' and `shark' cases produce parameterizations which are similar to each other but significantly different than the others cases. This is reasonable as dynamic and deforming geometries put unique burdens on the pressure projection step, as shown in the longer convergence times for these cases shown in Figure \ref{fig:simulation time}. Adopting a data-driven and auto-tuned approach enables these pressure-projection dominated cases to achieve significant accelerations.

\section{Conclusions}

This manuscript develops a successful data-driven method to accelerate the solution of discrete pressure Poisson systems found in incompressible flow simulations. Multi-grid methods are identified as linear convolutional encoder-decoder networks with optimal ($N\log N$) scaling, and the matrix coefficients are identified as the critical nonlinear input, not the projection source-term, embedding information such as boundary conditions. Mathematical constraints are used to further focus the learning capacity to a parameterized Jacobi-like Multi-grid smoother. The resulting data-driven solver is within 33\% of the minimum computational cost per V-cycle, and shown to accelerate classic Gauss-Sidel and SOR smoothers by 80-233\% on eleven simulation cases. Because of the focused learning capacity, the generalization is excellent, enabling 90\% effective transfer learning from a synthetic data-set and nearly 100\% transfer from reduced resolution simulations. 

The potential of machine learning advances to improve fluid dynamics is vast, but well-applied classical methods and constraints are needed to focus this potential. Wherever possible, this work has made the simplest choice in parameterization, leaving significant opportunities for future improvements. Many other flow solvers and linear systems are yet to be explored, and extensions to this basic data-driven approach could accelerate solutions for a wide array of discrete partial differential equations. 


\bibliography{mybibfile}

\end{document}